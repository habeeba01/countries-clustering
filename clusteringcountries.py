# -*- coding: utf-8 -*-
"""ClusteringCountries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDCWX7jss9n1zSHmMO2rWy4I4k_0Gp30

Opérations de nettoyage et de préparation des données :
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Chargement des données
data = pd.read_csv('Country-data.csv')

# Gestion des valeurs manquantes
data.fillna(data.mean(), inplace=True)  # Remplacer les valeurs manquantes par la moyenne des colonnes

# Vérification de la cohérence des valeurs
# Calcul des statistiques descriptives
summary_stats = data.describe()

# Affichage des statistiques pour détecter des valeurs incohérentes
print(summary_stats)

"""des visualisations exploratoires des données originales:"""

import matplotlib.pyplot as plt


# Histogrammes pour chaque indicateur
data.hist(figsize=(12, 10), bins=20)
plt.tight_layout()
plt.show()

# Graphiques en boîte pour chaque indicateur
data.plot(kind='box', figsize=(12, 8))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Diagrammes de dispersion pour explorer les corrélations
columns_to_plot = ['child_mort', 'exports', 'health', 'imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']
scatter_matrix = pd.plotting.scatter_matrix(data[columns_to_plot], figsize=(15, 15))
plt.tight_layout()
plt.show()

"""Ces statistiques descriptives montrent des échelles différentes pour chaque colonne. Certaines variables ont des plages de valeurs très différentes, par exemple, 'income' a une valeur maximale bien plus élevée que 'exports' ou 'health'. Ces différences dans les plages de valeurs peuvent influencer négativement les algorithmes qui s'appuient sur des calculs de distances.Donc , on doit appliquer la normalisation, elle est souvent recommandée dans le cadre de l'apprentissage automatique, surtout lorsque les variables ont des échelles très différentes. Elle permet de ramener toutes les variables à une échelle comparable, généralement entre 0 et 1, en préservant les relations entre les données. Cela facilite la convergence des algorithmes d'apprentissage et améliore souvent les performances.


"""

# Normalisation des échelles des indicateurs
scaler = MinMaxScaler()  # Initialise le scaler pour la normalisation
columns_to_normalize = ['child_mort', 'exports', 'health', 'imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']
data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])

# Vérification visuelle des données après le nettoyage et la préparation
print(data.head())

"""La visualisation des données normalisées"""

# Histogrammes pour chaque indicateur après la normalisation
data.hist(figsize=(12, 10), bins=20)
plt.tight_layout()
plt.show()

# Graphiques en boîte pour chaque indicateur après la normalisation
data.plot(kind='box', figsize=(12, 8))
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
# Diagrammes de dispersion après la normalisation
scatter_matrix = pd.plotting.scatter_matrix(data[columns_to_normalize], figsize=(15, 15))
plt.tight_layout()
plt.show()

"""Benchmarking entre les trois modèles choisis :

Sélectionnement des colonnes pertinentes pour le clustering
"""

# Sélectionner les colonnes pertinentes pour le clustering
X = data[['child_mort', 'exports', 'health', 'imports', 'income', 'inflation', 'life_expec', 'total_fer', 'gdpp']]

"""On commencera avec l'algorithme Kmeans

Kmeans se base sur le choix du K , on doit d'abord choisir notre cas . Le choix du k avec la méthode de coude :
"""

from sklearn.cluster import KMeans
import pandas as pd
import matplotlib.pyplot as plt

# Liste pour stocker les valeurs d'inertie
inertia = []

# Tester différentes valeurs de K
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)

# Afficher le graphique pour la méthode du coude
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Méthode du coude')
plt.xlabel('Nombre de clusters (K)')
plt.ylabel('Inertie')
plt.show()

"""Clustering avec K-means / 2-means"""

from sklearn.cluster import KMeans
import pandas as pd

# Initialisation du modèle K-Means avec k=2
kmeans = KMeans(n_clusters=2)

# Entraînement du modèle
kmeans.fit(X)

# Obtention des labels attribués à chaque point
labels = kmeans.labels_

# Affichage des clusters
print(labels)

"""Visualisation graphique"""

# Obtenir les labels et les centres des clusters
centers = kmeans.cluster_centers_

# Visualisation en 2D (en supposant deux attributs pour la simplicité)
plt.scatter(X['income'], X['child_mort'], c=labels, cmap='viridis')
plt.scatter(centers[:, 4], centers[:, 0], marker='o', c='red', s=200)  # Afficher les centres des clusters en rouge
plt.xlabel('Income')
plt.ylabel('Child Mortality')
plt.title('Clustering avec K-Means (k=2)')
plt.show()

"""Clustering avec Clustering Hierarchique

Le dendrogramme est un diagramme arborescent utilisé dans le clustering hiérarchique pour représenter les regroupements successifs des données. Il affiche la manière dont les points de données individuels sont regroupés au fil des étapes du processus de regroupement hiérarchique.Dans le cas du clustering hiérarchique, le dendrogramme présente une représentation visuelle des fusions successives de clusters et permet de déterminer le nombre de clusters approprié en identifiant la hauteur à laquelle couper le dendrogramme pour obtenir le nombre souhaité de clusters.
"""

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Calcul de la matrice de liaison
linkage_matrix = linkage(X, method='ward')

# Tracer le dendrogramme
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix)
plt.title('Dendrogramme')
plt.xlabel('Index des échantillons')
plt.ylabel('Distance')
plt.show()

"""Clustering avec nombre de clusters=2"""

from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram

# Initialiser le modèle avec 2 clusters
hierarchical = AgglomerativeClustering(n_clusters=2)
hierarchical.fit(X)

# Afficher le clustering
plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=hierarchical.labels_, cmap='viridis')
plt.title('Clustering Hiérarchique avec 2 Clusters')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""Clustering avec DBSCAN (Density-Based Spatial Clustering of Applications with Noise) :

Le choix des valeurs pour eps et min_samples dans DBSCAN est important pour obtenir des résultats pertinents
"""

from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt

# Calculer la distance entre les points
neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(X)
distances, indices = nbrs.kneighbors(X)

# Trier et afficher les distances
distances = sorted(distances[:, 1], reverse=True)
plt.figure(figsize=(8, 5))
plt.plot(range(len(distances[:100])), distances[:100])  # Limite les valeurs affichées à 100 pour améliorer la lisibilité
plt.xlabel('Points triés')
plt.ylabel('Distance au point voisin le plus proche')
plt.title('Méthode du genou pour choisir eps')
plt.show()

"""La courbe n'est pas visible , donc on va utiliser la méthode kneed"""

pip install kneed

from kneed import KneeLocator
import matplotlib.pyplot as plt

# Création du graphique
plt.plot(range(len(distances)), distances)
plt.xlabel('Points triés')
plt.ylabel('Distance au point voisin le plus proche')
plt.title('Méthode du genou pour choisir eps')

# Trouver le point du coude (le coude de la courbe)
kneedle = KneeLocator(range(len(distances)), distances, curve='convex', direction='decreasing')
knee_x = kneedle.knee

knee_y = kneedle.knee

# Afficher le point du coude sur le graphique
plt.vlines(knee_x, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')
plt.annotate(f'Point du coude: {knee_x}', xy=(knee_x, knee_y), xytext=(knee_x+10, knee_y-0.01),
             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.5'), fontsize=10)
plt.show()

# Récupérer la valeur du point du coude (epsilon)
epsilon = knee_x  # Utilisez knee_x pour l'estimation d'epsilon
print(f"Valeur estimée d'epsilon (point du coude) : {epsilon}")

"""DBSCAN :"""

from sklearn.cluster import DBSCAN

# Création du modèle DBSCAN avec eps=6
dbscan = DBSCAN(eps=3, min_samples=10)

# Application du modèle sur les données
dbscan.fit(X)  # Assurez-vous d'avoir défini X auparavant avec les caractéristiques pertinentes

# Affichage des labels des clusters résultants
print(dbscan.labels_)

"""On doit maintenant choisir le modèle qui convient le plus à nos données , on va choisir qu'entre K-means et Clustering Hierarchique vue que DBSCAN nous a fournit des -1 ,c'est à dire un mauvais choix de epsilon , donc on va se concentrer que sur les deux restés .

On effectuera une évaluation des résultats en utilisant le score de silhouette, une métrique permettant de mesurer la cohésion et la séparation des clusters. Les scores de silhouette plus proches de 1 indiquent une meilleure séparation entre les clusters. Vous pouvez ajuster les paramètres des algorithmes pour mieux convenir à vos données.
"""

from sklearn import metrics

# Évaluer les résultats avec des métriques de clustering
labels_kmeans = kmeans.labels_
labels_hierarchical = hierarchical.labels_
silhouette_kmeans = metrics.silhouette_score(X, labels_kmeans)
silhouette_hierarchical = metrics.silhouette_score(X, labels_hierarchical)

# Afficher les scores de silhouette
print(f"Score de silhouette pour K-Means : {silhouette_kmeans}")
print(f"Score de silhouette pour Clustering hiérarchique : {silhouette_hierarchical}")